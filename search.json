[
  {
    "objectID": "blog/cicd/index.html",
    "href": "blog/cicd/index.html",
    "title": "CI/CD",
    "section": "",
    "text": "In this tutorial, we’ll walk through the steps of setting up a continuous integration (CI) pipeline using GitHub Actions. We will also demonstrate how to achieve continuous deployment (CD) to two cloud-based platforms, a more specialized shinyapps.io and a more general AWS ECR and AWS App Runner.\nWe will specifically be focusing on deploying a Shiny app, but the concepts can easily be adapted for any application. Here is my repository as an example."
  },
  {
    "objectID": "blog/cicd/index.html#introduction",
    "href": "blog/cicd/index.html#introduction",
    "title": "CI/CD",
    "section": "",
    "text": "In this tutorial, we’ll walk through the steps of setting up a continuous integration (CI) pipeline using GitHub Actions. We will also demonstrate how to achieve continuous deployment (CD) to two cloud-based platforms, a more specialized shinyapps.io and a more general AWS ECR and AWS App Runner.\nWe will specifically be focusing on deploying a Shiny app, but the concepts can easily be adapted for any application. Here is my repository as an example."
  },
  {
    "objectID": "blog/cicd/index.html#prepare-for-deployment-on-shinyapps.io",
    "href": "blog/cicd/index.html#prepare-for-deployment-on-shinyapps.io",
    "title": "CI/CD",
    "section": "Prepare for deployment on Shinyapps.io",
    "text": "Prepare for deployment on Shinyapps.io\nTo host a Shiny app on shinyapps.io, you need to have a shinyapps.io account. You can sign up for a free account if you don’t have one.\n\nGet credentials\nFollow the steps below to get your shinyapps.io credentials.\n\nGo to your shinyapps.io account and log in.\nNavigate to Account and Tokens.\nClick on Add Token to generate a new token. Skip if you already have one.\nClick on Show to reveal the credentials.\n\n\n\nSetting up GitHub secrets\nFor secure communication between GitHub and the shinyapps.io, we need to store sensitive credentials in GitHub Secrets. You will need the following secrets: SHINYAPPS_USER, SHINYAPPS_TOKEN, and SHINYAPPS_SECRET.\nTo add secrets to your repository: 1. Go to the repository’s Settings. 2. Navigate to Secrets. 3. Click on New repository secret and add the relevant keys."
  },
  {
    "objectID": "blog/cicd/index.html#prepare-for-deployment-on-aws-app-runner",
    "href": "blog/cicd/index.html#prepare-for-deployment-on-aws-app-runner",
    "title": "CI/CD",
    "section": "Prepare for deployment on AWS App Runner",
    "text": "Prepare for deployment on AWS App Runner\nTo host a Shiny app on AWS App Runner, you need to have an AWS account with access to ECR and App Runner services. Set up an AWS account if necessary.\n\nPrepare a Docker file\nWe need a Dockerfile to build the Docker image of the Shiny app. Here is my example of a Dockerfile.\n\n# Use a base image with R and Shiny Server pre-installed\nFROM rocker/shiny-verse:latest\n\n# Install system libraries for geospatial analysis\nRUN apt-get update \\\n  && apt-get install -y --no-install-recommends \\\n    libudunits2-dev \\\n    libgdal-dev \\\n    libgeos-dev \\\n    libproj-dev \\\n    libmysqlclient-dev\n\n# Install R packages\nRUN R -e \"install.packages(c('shinyjs', 'shinyscreenshot', 'geosphere', 'raster', 'gstat', 'ggpubr', 'gridExtra', 'maps', 'rnpn','leaflet', 'terra','colorRamps', 'lubridate','digest','aws.s3','ptw','doSNOW','svglite','ggnewscale'), dependencies=TRUE)\"\n\n# Copy your Shiny app directory into the image\nCOPY &lt;app name&gt; /srv/shiny-server/&lt;app name&gt;\n\n# Expose the default Shiny Server port\nEXPOSE 3838\n\nWe can store this file at the root directory.\n\n\nConfigure AWS ECR\nWe then set up an image repository in AWS ECR to store the Docker image of the Shiny app.\n\nGo to the AWS Management Console and log in.\nSearch for Elastic Container Registry service and click Create repository. This is a private repository by default.\nSet the image repository name and click Create repository. The container image URI should be in the format of &lt;AWS account number&gt;.dkr.ecr.&lt;AWS region&gt;.amazonaws.com/&lt;image repository name&gt;:latest.\n\n\n\nConfigure AWS App Runner\nWe then set up a service in AWS App Runner that will run the Docker image of the Shiny app.\n\nGo to the AWS Management Console and log in.\nSearch for AWS App Runner service and click Create service.\nChoose Container registry as the repository type. Choose Amazon ECR as the provider. Set the container image URI by choosing the image repository name. Choose Manual for the deployment trigger. (This is because I want to keep my app paused most of the time and only resume and deploy as needed.) Choose using existing service role of AppRunnerECRAccessRole. Click Next.\nSet the service name, CPU and memory. Expose Port 3838 as this is what the Shiny app uses. Customize other configurations as needed. Click Next.\nReview the settings and click Create and deploy. Note that the service ARN is in the form of arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt;. Also note the default domain name of the service in the form of &lt;service ID&gt;.&lt;AWS region&gt;.awsapprunner.com.\n\n\n\nConfigure IAM role\nAs we are using GitHub Actions, we need to set up an IAM role that allows GitHub to assume a role for deploying to AWS services. Follow the steps below to set up the IAM role. reference\n\nGo to the AWS Management Console and log in.\nSearch for IAM service and go to the IAM dashboard.\nNavigate to Access management and Identity providers.\nIf there is not an identity provider called token.actions.githubusercontent.com, click on Add provider to create a new identity provider. For Provider type, choose OpenID Connect. For Provider URL, enter the URL of the GitHub OIDC IdP for this solution: https://token.actions.githubusercontent.com. For Audience, enter sts.amazonaws.com. This will allow the AWS Security Token Service (AWS STS) API to be called by this IdP. Add tag optionally. Click on Add provider.\nNavigate to Access management and Roles. Click on Create role. Choose Web identity as the trusted entity type. Select token.actions.githubusercontent.com as the identity provider. Choose sts.amazonaws.com as the audience. Enter GitHub organization, GitHub repository, and GitHub branch that match the app we intend to deploy.\nAdd the necessary permission policies to the role. For ECR and App Runner, you can use the AWSAppRunnerFullAccess, AWSAppRunnerServicePolicyForECRAccess, and AmazonEC2ContainerRegistryFullAccess policies. Click on Next.\nName the role and edit the trust policy. Here is my example.\n\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::&lt;aws account number&gt;:oidc-provider/token.actions.githubusercontent.com\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"token.actions.githubusercontent.com:aud\": \"sts.amazonaws.com\",\n                    \"token.actions.githubusercontent.com:sub\": \"repo:&lt;GitHub organization&gt;/&lt;GitHub repository&gt;:ref:refs/heads/&lt;GitHub branch&gt;\"\n                }\n            }\n        }\n    ]\n}\n\n\nReview the role and click on Create role.\nView the role and copy the ARN of the role.\n\n\n\nSetting up GitHub secrets\nWe need to store the ARN of the IAM role in GitHub Secrets. You will need the GitHub secret AWS_ROLE_ARN.\nTo add secrets to your repository: 1. Go to the repository’s Settings. 2. Navigate to Secrets. 3. Click on New repository secret and add the relevant keys."
  },
  {
    "objectID": "blog/cicd/index.html#creating-the-github-actions-workflow",
    "href": "blog/cicd/index.html#creating-the-github-actions-workflow",
    "title": "CI/CD",
    "section": "Creating the GitHub Actions workflow",
    "text": "Creating the GitHub Actions workflow\nNow that we have the necessary credentials, we can create the GitHub Actions workflow to automate the build and deployment process.\nCreate a .github/workflows/deploy.yml file in your repository. Below is my example. This YAML file defines a GitHub Actions workflow with two jobs:\n\nshinyapps-io: Deploys the Shiny app to shinyapps.io.\napp-runner: Builds a Docker image and pushes it to AWS ECR.\n\n\nname: Build and Deploy Shiny App to AWS\n\non:\n  push:\n    branches:\n      - main  # or the branch you want to deploy from\n  pull_request:\n    branches:\n      - main\n\n# Permission can be added at job level or workflow level    \npermissions:\n  id-token: write   # This is required for requesting the JWT\n  contents: read    # This is required for actions/checkout\n\njobs:\n  shinyapps-io:\n    runs-on: ubuntu-latest\n\n    container:\n      image: rocker/shiny-verse:latest # Because we are going to deploy to shinyapps.io using R code, we here use an image with R and some R packages pre-installed\n    \n    steps:\n    # Step 1: Checkout the code from GitHub\n    - name: Checkout repository\n      uses: actions/checkout@v3\n\n    # Step 2: Install system dependencies\n    - name: Install system dependencies # Note that this step is similar to one in the Dockerfile, because we are installing similar dependencies that the Shiny app requires\n      run: |\n        apt-get update\n        apt-get install -y --no-install-recommends \\\n          libudunits2-dev \\\n          libgdal-dev \\\n          libgeos-dev \\\n          libproj-dev \\\n          libmysqlclient-dev\n\n    # Step 3: Install R packages\n    - name: Install R packages # Note that this step is similar to one in the Dockerfile, because we are installing similar dependencies that the Shiny app requires\n      run: |\n        Rscript -e \"install.packages('rsconnect')\"\n        Rscript -e \"install.packages(c('shinyjs', 'shinyscreenshot', 'geosphere', 'raster', 'gstat', 'ggpubr', 'gridExtra', 'maps', 'rnpn','leaflet', 'terra','colorRamps', 'lubridate','digest','aws.s3','ptw','doSNOW','svglite','ggnewscale'), dependencies = TRUE)\"\n            \n    # Step 3: Deploy to shinyapps.io\n    - name: Deploy to shinyapps.io # We use a while loop here to retry the deployment in case of failure, often because that the previous deployment is still in progress\n      run: |\n      \n        retries=0\n        max_retries=10\n        while [[ $retries -lt $max_retries ]]\n        do\n          echo \"Attempt $((retries + 1)) of $max_retries\"\n          Rscript -e \"rsconnect::setAccountInfo(name='${{ secrets.SHINYAPPS_USER }}', token='${{ secrets.SHINYAPPS_TOKEN }}', secret='${{ secrets.SHINYAPPS_SECRET }}')\" && \\\n          Rscript -e \"rsconnect::deployApp(appDir = './phenowatch', appName = 'phenowatch', account = '${{ secrets.SHINYAPPS_USER }}', server = 'shinyapps.io', upload = T, forceUpdate = T)\" && {\n            echo \"Deployment successful.\"\n            break\n          } || {\n            echo \"Deployment failed, retrying in 5 minutes...\"\n            sleep 300 # wait for 5 minutes before retrying\n            retries=$((retries + 1))\n          }\n        done\n      shell: bash # This is required because we are in a container but we are using bash syntax \n      env:\n        RSCONNECT_USER: ${{ secrets.SHINYAPPS_USER }}\n        RSCONNECT_TOKEN: ${{ secrets.SHINYAPPS_TOKEN }}\n        RSCONNECT_SECRET: ${{ secrets.SHINYAPPS_SECRET }}\n\n  app-runner:\n    runs-on: ubuntu-latest\n    \n    steps:\n    # Step 1: Checkout the code from GitHub\n    - name: Checkout repository\n      uses: actions/checkout@v3\n        \n    # Step 2: Set up AWS CLI\n    - name: Set up AWS CLI\n      uses: aws-actions/configure-aws-credentials@v3\n      with:\n        role-to-assume: ${{ secrets.AWS_ROLE_ARN }}\n        role-session-name: GitHub_to_AWS_via_FederatedOIDC\n        aws-region: &lt;AWS region&gt;\n    \n    # Step 3: Build Docker image\n    - name: Set up Docker\n      uses: docker/setup-buildx-action@v3\n\n    - name: Build Docker image\n      run: |\n        docker build -f Dockerfile -t &lt;image repository name&gt;:latest .\n\n    # Step 4: Push Docker image to AWS ECR\n    - name: Log in to Amazon ECR\n      run: |\n        aws ecr get-login-password --region &lt;AWS region&gt; | docker login --username AWS --password-stdin &lt;AWS account number&gt;.dkr.ecr.&lt;AWS region&gt;.amazonaws.com\n\n    - name: Tag Docker image\n      run: |\n        docker tag &lt;image repository name&gt;:latest &lt;AWS account number&gt;.dkr.ecr.&lt;AWS region&gt;.amazonaws.com/&lt;image repository name&gt;:latest\n\n    - name: Push Docker image to ECR\n      run: |\n        docker push &lt;AWS account number&gt;.dkr.ecr.&lt;AWS region&gt;.amazonaws.com/&lt;image repository name&gt;:latest\n\n    # Step 5: Deploy to AWS App Runner\n    - name: Resume service if needed\n      run: |\n        \n        status=$(aws apprunner describe-service \\\n          --service-arn arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt; \\\n          --query 'Service.Status' --output text)\n        \n        if [[ \"$status\" == \"PAUSED\" ]]; then\n          echo \"Service is paused. Resuming now...\"\n          aws apprunner resume-service \\\n            --service-arn arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt; || {\n            echo \"Service resume failed, skipping this step.\"\n            exit 0\n          }\n          echo \"Service resumed.\"\n        else\n          echo \"Service is already running. No need to resume.\"\n        fi\n\n    - name: Update and deploy service\n      run: |\n      \n        retries=0\n        max_retries=10\n        while [[ $retries -lt $max_retries ]]\n        do\n          # Check if the service is in OPERATION_IN_PROGRESS state\n          status=$(aws apprunner describe-service --service-arn arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt; --query 'Service.Status' --output text)\n\n          echo \"Attempt $((retries + 1)) of $max_retries\"\n          if [[ \"$status\" == \"OPERATION_IN_PROGRESS\" ]]; then\n            echo \"Service is still in operation. Waiting for it to finish...\"\n            sleep 60 # Wait for 1 minute before checking again\n            retries=$((retries + 1))\n            continue\n          fi\n        \n          # Proceed with the update if not in OPERATION_IN_PROGRESS state\n          aws apprunner update-service \\\n            --service-arn arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt; \\\n            --source-configuration '{\"ImageRepository\": {\"ImageRepositoryType\": \"ECR\",\"ImageIdentifier\": \"&lt;AWS account number&gt;.dkr.ecr.&lt;AWS region&gt;.amazonaws.com/&lt;image repository name&gt;:latest\",\"ImageConfiguration\": {\"Port\": \"3838\"}}}'\n          echo \"Service updated.\"\n          \n          aws apprunner start-deployment \\\n            --service-arn arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt;\n          echo \"Service deployed.\"\n          break\n        done\n    \n    - name: Pause service\n      run: |\n\n        retries=0\n        max_retries=10\n        \n        while [[ $retries -lt $max_retries ]]\n        do\n          # Check if the service is in OPERATION_IN_PROGRESS state\n          status=$(aws apprunner describe-service --service-arn arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt; --query 'Service.Status' --output text)\n\n          echo \"Attempt $((retries + 1)) of $max_retries\"\n          if [[ \"$status\" == \"OPERATION_IN_PROGRESS\" ]]; then\n            echo \"Service is still in operation. Waiting for it to finish...\"\n            sleep 60 # Wait for 1 minute before checking again\n            retries=$((retries + 1))\n            continue\n          fi\n        \n          # Proceed with pausing if not in OPERATION_IN_PROGRESS state\n          aws apprunner pause-service \\\n          --service-arn arn:aws:apprunner:&lt;AWS region&gt;:&lt;AWS account number&gt;:service/&lt;app name&gt;/&lt;resource ID&gt; \n        \n          echo \"Service paused.\"\n          break\n        done\n\nPush the changes to out repository to trigger the workflow. As the workflow is running, inspect the status by navigating to the Actions tab in our repository and selecting the most recent workflow. A successful workflow will be indicated by a green checkmark. If the workflow fails, you can click on the job to view the logs and debug the issue."
  },
  {
    "objectID": "blog/cicd/index.html#check-deployed-apps-and-debug",
    "href": "blog/cicd/index.html#check-deployed-apps-and-debug",
    "title": "CI/CD",
    "section": "Check deployed apps and debug",
    "text": "Check deployed apps and debug\nFor the app hosted on shinyapps.io, you can navigate to your shinyapps.io account and view the deployed app. There you will find the url of the app in the form of https://&lt;username&gt;.shinyapps.io/&lt;appname&gt;. You can also use Logs tab to diagnose any issues.\nFor the app hosted on AWS App Runner, you can navigate to the AWS Management Console and go to the App Runner service. There you will find the url of the app in the form of https://&lt;service-id&gt;.&lt;AWS region&gt;.awsapprunner.com. You will need to add your Shiny app name to the end of the url https://&lt;service-id&gt;.&lt;AWS region&gt;.awsapprunner.com/&lt;shiny-app-name&gt;. You can also use the Deployment logs and Application logs to diagnose any issues."
  },
  {
    "objectID": "blog/phenology-sample-data/index.html",
    "href": "blog/phenology-sample-data/index.html",
    "title": "Phenology sample data",
    "section": "",
    "text": "Data prepared for Dr. Yang Chen’s class STATS605 in 2023 Fall, also good for others interested in exploring phenology data.\nData files and code for generating them can be found in this GitHub repo.\nThis blog gives some first steps reading and visualizing the data."
  },
  {
    "objectID": "blog/phenology-sample-data/index.html#meta-data-for-trees",
    "href": "blog/phenology-sample-data/index.html#meta-data-for-trees",
    "title": "Phenology sample data",
    "section": "Meta data for trees",
    "text": "Meta data for trees\n\nmetadata &lt;- read_csv(\"data/metadata.csv\")\n\nmetadata %&gt;%\n  head() %&gt;%\n  flextable::regulartable() %&gt;%\n  flextable::autofit() %&gt;%\n  flextable::fit_to_width(8)\n\nsitesite_latsite_lonidlatlonspeciesgrowth_formHARV42.54278-72.17213NEON.PLA.D01.HARV.0601642.54368-72.17330Quercus rubra L.Deciduous broadleafHARV42.54278-72.17213NEON.PLA.D01.HARV.0604642.54186-72.17096Quercus rubra L.Deciduous broadleafHARV42.54278-72.17213NEON.PLA.D01.HARV.0603442.54310-72.17097Quercus rubra L.Deciduous broadleafHARV42.54278-72.17213NEON.PLA.D01.HARV.0604142.54225-72.17102Quercus rubra L.Deciduous broadleafHARV42.54278-72.17213NEON.PLA.D01.HARV.0603942.54233-72.17089Quercus rubra L.Deciduous broadleafHARV42.54278-72.17213NEON.PLA.D01.HARV.0602942.54373-72.17132Quercus rubra L.Deciduous broadleaf\n\n\n\nmetadata %&gt;%\n  ggplot() +\n  geom_point(aes(x = lon, y = lat, col = species)) +\n  facet_wrap(. ~ site, scales = \"free\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  guides(col = \"none\") +\n  labs(\n    x = \"Longitude\",\n    y = \"Latitude\"\n  )\n\n\n\n\nCoordinates of tagged trees at two selected sites. Colors indicate species."
  },
  {
    "objectID": "blog/phenology-sample-data/index.html#discrete-phenology-data.",
    "href": "blog/phenology-sample-data/index.html#discrete-phenology-data.",
    "title": "Phenology sample data",
    "section": "Discrete phenology data.",
    "text": "Discrete phenology data.\n\nrnpn::npn_pheno_classes() %&gt;%\n  filter(id %in% 1:5) %&gt;%\n  select(id, name, description) %&gt;%\n  flextable::regulartable() %&gt;%\n  flextable::autofit() %&gt;%\n  flextable::fit_to_width(8)\n\nidnamedescription1Initial shoot or leaf growthInitiation of seasonal vegetative growth2Young leaves or needlesPresence of foliage still in process of maturing3Leaves or needlesPresence of live foliage4Colored leaves or needlesSenescent coloring of foliage5Falling leaves or needlesDropping of foliage\n\n\n\ndat_discrete &lt;- read_csv(\"data/discrete.csv\")\n\ndat_discrete %&gt;%\n  filter(status == \"yes\") %&gt;%\n  mutate(doy = date %&gt;% lubridate::mdy() %&gt;% lubridate::yday()) %&gt;%\n  arrange(phenophase_code) %&gt;%\n  mutate(phenophase = factor(phenophase, levels = unique(phenophase))) %&gt;%\n  ggplot() +\n  geom_segment(aes(x = doy, xend = doy, y = 0, yend = 1), alpha = 0.01, col = \"dark green\") +\n  facet_wrap(. ~ phenophase * site, ncol = 2) +\n  theme_classic() +\n  labs(\n    x = \"Day of year\",\n    y = \"\"\n  ) +\n  theme(axis.title.y = element_blank())\n\n\n\n\nTime of Yes observation of phenophase status.\n\n\n\n\n\ndat_discrete &lt;- read_csv(\"data/discrete.csv\")\n\ndat_discrete %&gt;%\n  filter(!is.na(intensity)) %&gt;%\n  mutate(doy = date %&gt;% lubridate::mdy() %&gt;% lubridate::yday()) %&gt;%\n  arrange(phenophase_code) %&gt;%\n  mutate(phenophase = factor(phenophase, levels = unique(phenophase))) %&gt;%\n  arrange(intensity_code) %&gt;%\n  mutate(intensity = factor(intensity, levels = unique(intensity))) %&gt;%\n  ggplot() +\n  geom_point(aes(x = doy, y = intensity), alpha = 0.01, col = \"dark green\") +\n  facet_wrap(. ~ phenophase * site, ncol = 2) +\n  theme_classic() +\n  labs(\n    x = \"Day of year\",\n    y = \"Intensity\"\n  )\n\n\n\n\nIntensity of phenophase status."
  },
  {
    "objectID": "blog/phenology-sample-data/index.html#continuous-phenology-data",
    "href": "blog/phenology-sample-data/index.html#continuous-phenology-data",
    "title": "Phenology sample data",
    "section": "Continuous phenology data",
    "text": "Continuous phenology data\n\ndat_continuous_3m &lt;- read_csv(\"data/continuous_3m.csv\")\n\ndat_continuous_3m %&gt;%\n  ggplot() +\n  geom_line(aes(x = date, y = evi, group = id), alpha = 0.01, col = \"dark green\") +\n  facet_wrap(. ~ site) +\n  theme_classic() +\n  labs(\n    x = \"Date\",\n    y = \"EVI\"\n  )\n\n\n\n\nEnhanced vegetation index at 3 m resolution.\n\n\n\n\n\ndat_continuous_500m &lt;- read_csv(\"data/continuous_500m.csv\")\n\ndat_continuous_500m %&gt;%\n  ggplot() +\n  geom_line(aes(x = date, y = evi), col = \"dark green\") +\n  facet_wrap(. ~ site) +\n  theme_classic() +\n  labs(\n    x = \"Date\",\n    y = \"EVI\"\n  )\n\n\n\n\nEnhanced vegetation index at 500 m resolution."
  },
  {
    "objectID": "blog/phenology-sample-data/index.html#weather-data",
    "href": "blog/phenology-sample-data/index.html#weather-data",
    "title": "Phenology sample data",
    "section": "Weather data",
    "text": "Weather data\n\ndat_weather &lt;- read_csv(\"data/weather.csv\")\n\ndat_weather %&gt;%\n  gather(key = \"variable\", value = \"value\", -site, -date) %&gt;%\n  ggplot() +\n  geom_line(aes(x = date, y = value, col = variable)) +\n  facet_wrap(. ~ variable * site, scales = \"free_y\", ncol = 2) +\n  theme_classic() +\n  labs(\n    x = \"Date\",\n    y = \"Value\"\n  )\n\n\n\n\nWeather variables at two sites."
  },
  {
    "objectID": "blog/reproducible-workflow/index.html",
    "href": "blog/reproducible-workflow/index.html",
    "title": "Reproducible workflow",
    "section": "",
    "text": "A research project is computationally reproducible if a second investigator (including you in the future) can recreate the final reported results of the project, including key quantitative findings, tables, and figures, given only a set of files and written instructions.\nKitzes, J., Turek, D., & Deniz, F. (Eds.). (2018). The Practice of Reproducible Research: Case Studies and Lessons from the Data-Intensive Sciences. Oakland, CA: University of California Press. Link to book\nIn this blog, I intend not to repeat the content of the well-written book, but to collate some notes and resources for us to practice reproducible workflow. (I’m writing mainly for R users here, but a lot applies to Python users.)\nAs we gradually adopt the practices below, we can make our work reproducible for ourselves, our research team, colleagues, others in the research field, and even the general public."
  },
  {
    "objectID": "blog/reproducible-workflow/index.html#working-environment",
    "href": "blog/reproducible-workflow/index.html#working-environment",
    "title": "Reproducible workflow",
    "section": "Working environment",
    "text": "Working environment\nThere are many restrictions working only on personal computers. Others, as well as ourselves in a different setting, might not be able to reproduce our work. It is important to be able to set up a working environment that can be accessed easily from most places. I recommend using cloud computing and cloud storage. * Cloud computing * At UofM, there is Great Lakes HPC Cluster. Check my documentation prepared for Zhu Lab for usage. There are limits on rates and quota. * AWS EC2 instances are fairly popular and easy to use. Recommend. You have full control over your instance. Not free, but we can apply for research credit. * Microsoft Azure VMs are similar to EC2. * Google Colab is a hosted Jupyter Notebook service. Free. Good for machine learning tasks. * Cyverse funded by NSF is an Open Science Workspace. They host lots of workshops. * Cloud storage * At UofM, there is Turbo. Check my documentation prepared for Zhu Lab for usage. There are limits on rates and quota. * Google Drive provides large storage for many university accounts and it can be mounted easily as a local drive. It can be accessed easily from R after authentication. Not standard practice for sensitive data.\n\n# Install the googledrive package.\nif (!require(googledrive)) {\n  install.packages(\"googledrive\")\n}\n\n# Load the package\nlibrary(googledrive)\n\n# OAuth2.0 flow, It would require browser authentication\ndrive_auth()\n\n# List files in Google Drive\ndrive_ls()\n\n# Download a file from Google Drive by specifying its name or id\ndrive_download(\"example.txt\")\n\n\nAWS S3 buckets are fairly popular and easy to use. Recommend. You can also access it in R.\n\n\n# Install aws.s3 package if not already installed\nif (!require(aws.s3)) {\n  install.packages(\"aws.s3\")\n}\n\n# Load aws.s3 library\nlibrary(\"aws.s3\")\n\n# Set up S3 access credentials\nSys.setenv(\n  \"AWS_ACCESS_KEY_ID\" = \"your_aws_access_key_here\",\n  \"AWS_SECRET_ACCESS_KEY\" = \"your_aws_secret_access_key_here\",\n  \"AWS_DEFAULT_REGION\" = \"us-east-1\"\n)\n\n# Get object list in the bucket\nbucketlist &lt;- get_bucket(bucket = \"your_bucket_name_here\")\n\n# Read a csv file from the bucket\ndata &lt;- s3read_using(FUN = read.csv, object = \"your_file_name_here\", bucket = \"your_bucket_name_here\")\n\n# View dataframe\nView(data)\n\n\nMicrosoft Azure Blob storage is similar.\nReminders\n\nIt is important to use relative paths in your code so that they still work when you changing your working environment.\nUse the correct working directory. Using R projects can help with that.\nUse symbolic links when using external folders for storage. It is like a shortcut. You won’t have to change all paths in your code, but just have to set up symbolic link again when changing computing environment. This should work in Linux but not Windows.\n\n\n\n  ln -s [source] [destination]"
  },
  {
    "objectID": "blog/reproducible-workflow/index.html#documentation-and-version-control",
    "href": "blog/reproducible-workflow/index.html#documentation-and-version-control",
    "title": "Reproducible workflow",
    "section": "Documentation and version control",
    "text": "Documentation and version control\nWe make a lot of changes in our project along the way and we can easily lose track. We want to keep the deprecated work but we don’t want to make our working directory too cluttered. We want to be able to retrieve previous work and understand what we were doing. * Documentation * Always write comments for your code. * Write metadata. For example, use the EML template for ecological data. * Name your functions and objects informatively. Here is what I do. * Key functions are named in the fashion of action_object_suffix. Types of actions can be: down, tidy, read, calc, plot, test, summ. Objects are the object of the action, such as “climate,” or “plant”. Suffix represent the level of subsetting or a variation, such as “individual.” There can be multiple suffix. * Key R objects are named in the fashion of prefix_class_object_suffix. Types of classes can be: dat, v, ras, df, gg. Prefix “ls” means list. Object and suffix similar to above. * Write project meeting notes, perhaps in Google docs. They now have a shortcut to generate headings for meeting notes. Type @meeting notes and choose the meeting you are taking notes for. * Version control * GitHub is strongly recommended. See my previous blog for a quick start. * Use different branches for different stages of the project. Here is what we recommend. * An “analysis” branch for preliminary analysis. This branch may be messy. * A “manuscript” branch to reproduce all analyses used in the manuscript during the submission stage. Code should be tidy, preferably made into package form in this branch. Data folder might be bulky, so it might be an external folder mounted with a symbolic link. An example of the structure looks like this.\n\n    your_package/\n    |-- DESCRIPTION\n    |-- NAMESPACE\n    |-- R/\n    |   |-- file1.R\n    |   |-- file2.R\n    |-- man/\n    |   |-- file1.Rd\n    |   |-- file2.Rd\n    |-- alldata/\n    |   |-- input/\n    |   |-- intermediate/\n    |   |-- output/\n    |-- vignettes/\n    |-- LICENSE\n\n* A \"release\" branch to provide a concise reproducible version for the public after publication. Code should be a subset of the ones in the \"manuscript\" branch, put into package. Small data files should be included in the package. There should not be dependency on external data folders. An example of the structure should look like this.\n\n    your_package/\n    |-- DESCRIPTION\n    |-- NAMESPACE\n    |-- R/\n    |   |-- file1.R\n    |   |-- file2.R\n    |-- man/\n    |   |-- file1.Rd\n    |   |-- file2.Rd\n    |-- data/\n    |-- inst/\n    |   |-- extdata/\n    |   |-- figures/\n    |   |-- doc/\n    |-- Meta/\n    |-- vignettes/\n    |-- LICENSE\n\n\nUse “release” in GitHub to mark important versions that you might come back to. Try to write release notes."
  },
  {
    "objectID": "blog/reproducible-workflow/index.html#sharing-work",
    "href": "blog/reproducible-workflow/index.html#sharing-work",
    "title": "Reproducible workflow",
    "section": "Sharing work",
    "text": "Sharing work\nThis is a key part of computational reproducibility. * Sharing code * Again, GitHub (or Bitbucket). * Sharing code and data * For publications, there are many repositories that allow uploading of bundles of code and data for peer-review and publication. They also give you a permanent DOI for your code and data. Popular ones are Zenodo, Figshare, and Dryad. Zenodo and Figshare have easy interactions with GitHub. * Sharing working environment, code, and data * This is sometimes important if your project uses some software or packages that are not commonly available, or requires specific versions of them. As you know, package installation can be very frustrating and can discourage others from reproducing your work. * Provide session info in readme.\n\nsessionInfo()\n\n\nUse established base images such as rocker.\nBuild your own docker images and upload to Docker Hub. You will first need a Dockerfile. After that, you can follow the steps below.\n\n\n  # build\n  docker build -t &lt;your_image_name&gt; .\n  \n  # test\n  docker run -it &lt;your_image_name&gt;\n  \n  # login\n  docker login\n  \n  # tag\n  docker tag &lt;your_image_name&gt; &lt;your_docker_username&gt;/&lt;your_docker_repo&gt;:&lt;tag&gt;\n  \n  \n  # push\n  docker push &lt;your_docker_username&gt;/&lt;your_docker_repo&gt;:&lt;tag&gt;"
  },
  {
    "objectID": "blog/reproducible-workflow/index.html#automating-workflow",
    "href": "blog/reproducible-workflow/index.html#automating-workflow",
    "title": "Reproducible workflow",
    "section": "Automating workflow",
    "text": "Automating workflow\nEven with all code and data, we might not know the exact order to load and run them.\n\nWrite scripts as functions and make packages\n\nPackage devtools and roxygens are powerful tools for package development. You can also use the GUI of RStudio.\n\n\n\n# create package (from parent directory)\ndevtools::create(\"myPackage\")\n\n# generate the documentation\ndevtools::document(\"myPackage\")\n\n# install and test package\ndevtools::install(\"myPackage\")\nlibrary(myPackage)\n\n# build package\ndevtools::build(\"myPackage\")\n\n\nSome resources: R package Primer\nWrite R vignettes (from R markdown)\n\n\ndevtools::build_vignettes()\n\n# skip some steps if I have already generated cached vignettes and installed package\ndevtools::build_vignettes(clean = F, install = F)\n\nbrowseVignettes(\"myPackage\")\n\n\nExecutes a series of tasks\n\nWrite bash scripts\n\nUse cron jobs to trigger them regularly.\n\nMakefile\nSnakemake\n\nGitHub Actions\n\nBuild, test, and deploy our code.\nRun on GitHub’s virtual machines.\nCheck out their recommended workflows and documentation."
  },
  {
    "objectID": "blog/reproducible-workflow/index.html#reporting-results",
    "href": "blog/reproducible-workflow/index.html#reporting-results",
    "title": "Reproducible workflow",
    "section": "Reporting results",
    "text": "Reporting results\nThere is still some gap between the results from running our code and what we present in the manuscript. * R markdown * One-page reports * You can render (knit) R markdown documents easily using RStudio. Uses package knitr. * Introduction * Publish R markdown to RStudio Connect * Bookdown * Several structured pages, written in R markdown * Introduction * You can publish it to bookdown.org. * Blogdown * A series of blogs, also written in R markdown * This current webpage is published through blogdown. Check my previous blog for a quick start. * Introduction * You can publish it as a GitHub page or on Netlify."
  },
  {
    "objectID": "blog/blogdown/index.html",
    "href": "blog/blogdown/index.html",
    "title": "Blogdown",
    "section": "",
    "text": "This is a practical introduction to using blogdown, a package for creating websites and blogs with R Markdown. It is suitable for creating project websites and personal blogs.\nRead blogdown instructions here."
  },
  {
    "objectID": "blog/blogdown/index.html#set-up-a-blogdown-project",
    "href": "blog/blogdown/index.html#set-up-a-blogdown-project",
    "title": "Blogdown",
    "section": "Set up a blogdown project",
    "text": "Set up a blogdown project\nCreate an R project. It is strongly recommended to make the working directory a GitHub repo for version control and deployment.\nMake this folder a blogdown site using R package blogdown.\n\ninstall.packages(\"blogdown\")\nblogdown::new_site()\n\nHUGO may be installed automatically, but manually install HUGO if necessary. The example below is for windows. See full installation instructions here.\n\nwinget install Hugo.Hugo.Extended\n\nIf the installed HUGO cannot be found, set PATH to include the path for HUGO\n\nnew_path &lt;- \"[path where HUGO is installed]\"\ncurrent_path &lt;- Sys.getenv(\"PATH\")\nnew_path_list &lt;- c(current_path, new_path)\nSys.setenv(PATH = paste(new_path_list, collapse = \";\"))\nSys.getenv(\"PATH\")\n\nYour working directory should have new folders and files used for the blogdown site.\nA sample site can be previewed in the Viewer.\nStop and serve your site to preview when you make further changes.\n\nblogdown::stop_server()\nblogdown::serve_site()"
  },
  {
    "objectID": "blog/blogdown/index.html#change-blog-content",
    "href": "blog/blogdown/index.html#change-blog-content",
    "title": "Blogdown",
    "section": "Change blog content",
    "text": "Change blog content\nCreate a new R markdown for your new blog post. Save it as content/post/[YYYY]-[MM]-[DD]-[title]/index.Rmd following the other examples.\nDelete the other example blog post folders.\nIf there are files needed to knit this R markdown, save the files in the same folder as the markdown."
  },
  {
    "objectID": "blog/blogdown/index.html#customize-website-elements",
    "href": "blog/blogdown/index.html#customize-website-elements",
    "title": "Blogdown",
    "section": "Customize website elements",
    "text": "Customize website elements\nThe customization below applies to the theme “lithium,” but other themes might be customized in a similar fashion.\nChange content in “About” page, found in content/about.md.\nChange GitHub link to the link of this project in config.yaml.\nChange Twitter link also in config.yaml. You can also delete this menu item or add other ones.\nChange logo (also the “Home” button). Save your desired logo at themes/hugo-lithium/static/images/, where the original logo is saved. Change config.yaml params: logo: url: to the new file name.\nChange favicon (tab preview image). Save your desired logo at themes/hugo-lithium/static/, where the original favicon is saved. Change config.yaml params: favicon: to the new file name.\nChange website title in config.yaml title:.\nCredit: Logo of this site was downloaded here."
  },
  {
    "objectID": "blog/blogdown/index.html#deploy-site-as-a-github-page",
    "href": "blog/blogdown/index.html#deploy-site-as-a-github-page",
    "title": "Blogdown",
    "section": "Deploy site as a GitHub page",
    "text": "Deploy site as a GitHub page\nYou can deploy either on GitHub or Netlify. Only GitHub deployment is discussed here.\nBuild site locally.\n\nblogdown::build_site()\n\nBy default, the built webpages are stored in public/, but we want to change the directory name to docs/ for GitHub deployment. To do this, add the line of “publishDir: docs” in config.yaml. Build site again. Don’t forget to delete public/.\nChange baseurl from relative url to absolute url of the GitHub page, otherwise GitHub page does not get the paths right. To do this, change baseurl: in line 1 of config.yaml from “/” to “https://[username].github.io/[repo name]/”.\nTo avoid some bugs in navigation using the “Home” button, I made changes to the themes/hugo-lithium/layouts/partials/nav.html file. I changed the {{ “/” | relURL }} in line 2 to {{ .Site.BaseURL }}.\nTo fix website title display, I made changes to the themes/hugo-lithium/layouts/partials/head.html file in two places. I changed both {{ if eq .RelPermalink “/” }} in {{ if eq .Permalink .Site.BaseURL }}.\nTo make sure all content in docs/ are pushed to GitHub, comment out docs/ in .gitignore if applicable. Push changes to GitHub. Remember to build site again before pushing.\nGo to repo on the GitHub website. Settings &gt; Pages &gt; Build and deployment. Select main branch (or whichever you want), /docs, and save.\nThe website should deploy automatically upon pushing. If deployed successfully, there should be a green tick next to the commit hash. Otherwise, there is a red cross. Click on the red cross or green tick to inspect more details about the deployment.\nThe website url should be “https://[username].github.io/[repo name]/”. You can also find the link in the last step of deployment details."
  },
  {
    "objectID": "research/b4warmed/index.html",
    "href": "research/b4warmed/index.html",
    "title": "Forest tree phenology in a global change experiment",
    "section": "",
    "text": "Warming advances leaf onset and shoot elongation in forest trees.\nResponses in duration and magnitude of spring development to warming are species-specific.\nClimate change does not lead to enhanced tree growth in most species.\nDifferential growth response may lead to shifts in forest species composition."
  },
  {
    "objectID": "research/b4warmed/index.html#highlights",
    "href": "research/b4warmed/index.html#highlights",
    "title": "Forest tree phenology in a global change experiment",
    "section": "",
    "text": "Warming advances leaf onset and shoot elongation in forest trees.\nResponses in duration and magnitude of spring development to warming are species-specific.\nClimate change does not lead to enhanced tree growth in most species.\nDifferential growth response may lead to shifts in forest species composition."
  },
  {
    "objectID": "research/b4warmed/index.html#abstract",
    "href": "research/b4warmed/index.html#abstract",
    "title": "Forest tree phenology in a global change experiment",
    "section": "Abstract",
    "text": "Abstract\nClimate change is causing trees to start growing earlier in the spring, but it’s not clear if this leads to more growth overall. This matters for how much carbon forests can store and which tree species will dominate in the future. Our study looked at how warming and reduced rainfall affect the timing, duration, and amount of spring growth in temperate and boreal forest trees, using data from a long-term global change experiment. We found that higher temperatures make trees leaf out and grow shoots earlier, but different species have different responses when it comes to how long this growth lasts and how much growth they can have in one spring. Reduced rainfall generally slowed down growth. Our results challenge the idea that climate change will boost tree growth, suggesting a possible limit to forests’ ability to absorb carbon dioxide. Also, the combination of warming and drying might change which species are more competitive, favoring those that can better adapt to these conditions."
  },
  {
    "objectID": "research/b4warmed/index.html#b4warmed-global-change-experiment",
    "href": "research/b4warmed/index.html#b4warmed-global-change-experiment",
    "title": "Forest tree phenology in a global change experiment",
    "section": "B4WarmED Global Change Experiment",
    "text": "B4WarmED Global Change Experiment\n\n\n\nA plot at B4WarmED experiment\n\n\n\n\n\nPhenology data collected from the experiment in different forms."
  },
  {
    "objectID": "research/b4warmed/index.html#climate-change-impacts-on-on-phenophase-status",
    "href": "research/b4warmed/index.html#climate-change-impacts-on-on-phenophase-status",
    "title": "Forest tree phenology in a global change experiment",
    "section": "Climate change impacts on on phenophase status",
    "text": "Climate change impacts on on phenophase status\n\n\n\nDifference in time of budburst induced by warming and drying treatments for red oak and red maple."
  },
  {
    "objectID": "research/b4warmed/index.html#climate-change-impacts-on-shoot-growth",
    "href": "research/b4warmed/index.html#climate-change-impacts-on-shoot-growth",
    "title": "Forest tree phenology in a global change experiment",
    "section": "Climate change impacts on shoot growth",
    "text": "Climate change impacts on shoot growth\n\n\n\nFitting a hierarchical Bayesian model based on a logistic functional shape to model shoot growth in spring.\n\n\n\n\n\nEffect sizes of warming, drying, and closed canopy on shoot growth in spring for red oak."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Forest tree phenology in a global change experiment\n\n\nWarming makes forest trees grow earlier in the spring, but often not taller\n\n\n\nChanging phenology\n\n\nFirst-author\n\n\n\nManuscript in preparation, invited oral presentation at AGU 2024\n\n\n\n\n\nAug 31, 2024\n\n\nYiluan Song, Yang Chen, Rebecca A. Montgomery, Peter B. Reich, Artur Stefanski, Raimundo Bermudez Villanueva, Kai Zhu\n\n\n\n\n\n\n\n\n\n\n\n\nNEON phenology forecasting community challenge\n\n\nPredicting spring phenology in deciduous broadleaf forests\n\n\n\nEcological forecasting\n\n\nFeatured\n\n\n\nAgricultural and Forest Meteorology 2024 (Paper led by Dr. Kathryn I. Wheeler)\n\n\n\n\n\nFeb 15, 2024\n\n\nYiluan Song, Uttam Bhat, Stephan B. Munch, Kai Zhu, as a team in the forecasting challenge\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yiluan Song",
    "section": "",
    "text": "I’m a Schmidt AI in Science Postdoctoral Fellow at the University of Michigan.\nMy research program operates at the intersection of environmental data science and global change biology. I aim to provide both theoretical insights and practical solutions to the challenges posed by our rapidly changing world.\nOver the years, I have committed to advancing our understanding of how climate change impacts phenology and biodiversity, with implications for both natural and social systems. My current work further integrates AI methodologies with ecological research. My research vision centers on three key pillars: harnessing big data, informing decision-making, and advancing ecological theories.\n\n\n\n\n\n\nChanging phenology\nChanging biodiversity\nEcological forecasting\nAllergen phenology"
  },
  {
    "objectID": "index.html#bio",
    "href": "index.html#bio",
    "title": "Yiluan Song",
    "section": "",
    "text": "I’m a Schmidt AI in Science Postdoctoral Fellow at the University of Michigan.\nMy research program operates at the intersection of environmental data science and global change biology. I aim to provide both theoretical insights and practical solutions to the challenges posed by our rapidly changing world.\nOver the years, I have committed to advancing our understanding of how climate change impacts phenology and biodiversity, with implications for both natural and social systems. My current work further integrates AI methodologies with ecological research. My research vision centers on three key pillars: harnessing big data, informing decision-making, and advancing ecological theories."
  },
  {
    "objectID": "index.html#research-directions",
    "href": "index.html#research-directions",
    "title": "Yiluan Song",
    "section": "",
    "text": "Changing phenology\nChanging biodiversity\nEcological forecasting\nAllergen phenology"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Secret removal from GitHub repository\n\n\n\n\n\n\nWorkflow\n\n\n\n\n\n\n\n\n\nMar 31, 2025\n\n\nYiluan Song\n\n\n\n\n\n\n\n\n\n\n\n\nCI/CD\n\n\n\n\n\n\nWorkflow\n\n\n\n\n\n\n\n\n\nJan 28, 2025\n\n\nYiluan Song\n\n\n\n\n\n\n\n\n\n\n\n\nNon-metric Multidimensional Scaling (NMDS)\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nYiluan Song\n\n\n\n\n\n\n\n\n\n\n\n\nReproducible workflow\n\n\n\n\n\n\nWorkflow\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nYiluan Song\n\n\n\n\n\n\n\n\n\n\n\n\nPhenology sample data\n\n\n\n\n\n\nEcology\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nYiluan Song\n\n\n\n\n\n\n\n\n\n\n\n\nGitHub\n\n\n\n\n\n\nWorkflow\n\n\n\n\n\n\n\n\n\nSep 10, 2023\n\n\nYiluan Song\n\n\n\n\n\n\n\n\n\n\n\n\nBlogdown\n\n\n\n\n\n\nPresentation\n\n\nWorkflow\n\n\n\n\n\n\n\n\n\nSep 9, 2023\n\n\nYiluan Song\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "research/efi/index.html",
    "href": "research/efi/index.html",
    "title": "NEON phenology forecasting community challenge",
    "section": "",
    "text": "We participated in a community forecast challenge to predict daily plant greenness with our model GPEDM out of 18 models.\nForecasts across teams showed that historical means of greenness on each day of year were difficult to outperform.\nGPEDM had the best performance among similar models that are data-driven, dynamic (includes previous state), and use covariates.\nFor the study sites, several static models or models without covariates had better performances than GPEDM in predicting greenness values.\nGPEDM showed its advantage in accurately predicting the ecologically important greenup transition dates."
  },
  {
    "objectID": "research/efi/index.html#highlights",
    "href": "research/efi/index.html#highlights",
    "title": "NEON phenology forecasting community challenge",
    "section": "",
    "text": "We participated in a community forecast challenge to predict daily plant greenness with our model GPEDM out of 18 models.\nForecasts across teams showed that historical means of greenness on each day of year were difficult to outperform.\nGPEDM had the best performance among similar models that are data-driven, dynamic (includes previous state), and use covariates.\nFor the study sites, several static models or models without covariates had better performances than GPEDM in predicting greenness values.\nGPEDM showed its advantage in accurately predicting the ecologically important greenup transition dates."
  },
  {
    "objectID": "research/efi/index.html#the-forecasting-challenge",
    "href": "research/efi/index.html#the-forecasting-challenge",
    "title": "NEON phenology forecasting community challenge",
    "section": "The forecasting challenge",
    "text": "The forecasting challenge\n\n\n\nLocations of selected sites and the National Phenology Network’s Historical Annual Spring Indices Anomaly for First Leaf product during the study year of 2021 compared to the 1991–2020 average.\n\n\n\n\n\nThe specific days that each model forecasted and the days that each team submitted forecasts on."
  },
  {
    "objectID": "research/efi/index.html#our-model",
    "href": "research/efi/index.html#our-model",
    "title": "NEON phenology forecasting community challenge",
    "section": "Our model",
    "text": "Our model\n\n\n\nGaussian Process time-delay embedding model with a spatial covaraince function layer and a temporal covariance function layer Sugihara et al., 2012"
  },
  {
    "objectID": "research/efi/index.html#example-of-forecasts",
    "href": "research/efi/index.html#example-of-forecasts",
    "title": "NEON phenology forecasting community challenge",
    "section": "Example of forecasts",
    "text": "Example of forecasts\n\n\n\nAn example of forecasted greenness values (GCC) submitted by teams on 11 May 2021 for Harvard Forest, with PhenoCan images on dates of 15% and 85% greenup."
  },
  {
    "objectID": "research/efi/index.html#model-evaluation",
    "href": "research/efi/index.html#model-evaluation",
    "title": "NEON phenology forecasting community challenge",
    "section": "Model evaluation",
    "text": "Model evaluation\n\n\n\nMean predictive skill by model relative to the day of year (DOY) Mean null model.\n\n\n\n“For the 15 %, 50 %, and 85 % greenup transition dates, PEG, GPEDM, and greenbears_gams beat the DOY Mean model furthest out”\n\n\n\n\nForecast horizon, or number of days before the transition dates that each forecast model did better at forecasting greenness (GCC) than day of year mean model across the range of all sites."
  },
  {
    "objectID": "blog/secret-removal/index.html",
    "href": "blog/secret-removal/index.html",
    "title": "Secret removal from GitHub repository",
    "section": "",
    "text": "Recently, I received a notification that my GitHub repository contains a secret. A secret is a sensitive information that should not be shared publicly, including passwords, API keys, and tokens. We should avoid storing secrets in a GitHub repository to prevent unauthorized access, even when the repository is private. It is not sufficient to simply delete the secret from the repository, as the commit history may still contain the secret. In this blog, I will demonstrate how to remove secrets from a GitHub repository."
  },
  {
    "objectID": "blog/secret-removal/index.html#introduction",
    "href": "blog/secret-removal/index.html#introduction",
    "title": "Secret removal from GitHub repository",
    "section": "",
    "text": "Recently, I received a notification that my GitHub repository contains a secret. A secret is a sensitive information that should not be shared publicly, including passwords, API keys, and tokens. We should avoid storing secrets in a GitHub repository to prevent unauthorized access, even when the repository is private. It is not sufficient to simply delete the secret from the repository, as the commit history may still contain the secret. In this blog, I will demonstrate how to remove secrets from a GitHub repository."
  },
  {
    "objectID": "blog/secret-removal/index.html#scan-for-secrets",
    "href": "blog/secret-removal/index.html#scan-for-secrets",
    "title": "Secret removal from GitHub repository",
    "section": "Scan for secrets",
    "text": "Scan for secrets\nI use GitGuardian to scan for secrets in my GitHub repositories. GitGuardian can be integrated with GitHub to automatically scan for secrets in repositories.\nYou would need to sign up for GitGuardian and authorize access to your GitHub repositories. Once the integration is set up, GitGuardian will scan the repositories you own (could be those in an organization) for secrets.\nIn the “Monitored perimeter” section, I can see the repositories that have been scanned, with their health status being “At risk” or “Safe.”\nIf you click on a repository, you can see the secrets that have been detected. For each secret, you can see the file path, the type of secret, and the commit that introduced the secret."
  },
  {
    "objectID": "blog/secret-removal/index.html#remove-secrets",
    "href": "blog/secret-removal/index.html#remove-secrets",
    "title": "Secret removal from GitHub repository",
    "section": "Remove secrets",
    "text": "Remove secrets\nYou can still remove the leaked API key from your Git commit history, but simply deleting or modifying the file in a new commit won’t be enough.\nWe can use the tool git-filter-repo to remove the secret from the commit history. First, install the tool by running the following command in the terminal.\n\npip install git-filter-repo\n\nAlternatively, you can install the tool directly to the folder of your choice.\n\nmkdir -p &lt;path for installation&gt;\ncd &lt;path for installation&gt;\ncurl -LO https://raw.githubusercontent.com/newren/git-filter-repo/main/git-filter-repo\nchmod +x git-filter-repo\nexport PATH=&lt;path for installation&gt;:$PATH\n\nCheck if you have the tool installed correctly.\n\ngit filter-repo --version\n\nYou need to clone the repository as a mirror repository. Note that if you already have this repository cloned in your local environment and have been working on it, you should push your changes and delete this local clone, because we need to work on a fresh clone of the repository.\n\ncd &lt;where you usually store your GitHub repositories&gt;\ngit clone --mirror https://github.com/&lt;your username or organization name&gt;/&lt;your repo&gt;.git\n\nWhen you use the git clone --mirror command, you might notice that the repo is detached from the remote repository. This is normal.\nWe will then go into this repository and use the git filter-repo command to remove the secret. Replace &lt;your-secret-key&gt; with the actual secret you want to remove. You should have obtained the leaked secret from GitGuardian just now. It could be a password, an API key, or a token. If you have multiple secrets to remove, repeat the git filter-repo command for each secret.\n\ncd &lt;your repo&gt;\ngit filter-repo --replace-text &lt;(echo \"&lt;your-secret-key&gt;==&gt;REMOVED\")\n\nNow that we have cleaned the local repository, we will re-add the remote and force push the cleaned repository.\n\ngit remote add origin https://github.com/&lt;your username or organization name&gt;/&lt;your repo&gt;.git\ngit push --force --all"
  },
  {
    "objectID": "blog/secret-removal/index.html#wrap-up",
    "href": "blog/secret-removal/index.html#wrap-up",
    "title": "Secret removal from GitHub repository",
    "section": "Wrap up",
    "text": "Wrap up\nYou are strongly suggested to revoke the leaked secret and generate a new one. Do this in the service where the secret was generated. For example, if it was an API key for Google Cloud, you should revoke the key in the Google Cloud Console and generate a new one.\nBack in your repository, You should update the secret, as well as how it is used. Do not write the secret in your code. You might want to use environment variables or a secret manager to store the secret.\nHere is an example of using environment variables to connect to an account for shinyapps.io in R.\n\n# Load existing .env file (if it exists)\ndotenv::load_dot_env(\".env\")\n\n# Check each required variable and prompt if missing\nname &lt;- check_and_prompt_env(\"RSCONNECT_NAME\", \"Enter shinyapps.io name: \")\ntoken &lt;- check_and_prompt_env(\"RSCONNECT_TOKEN\", \"Enter shinyapps.io token: \")\nsecret &lt;- check_and_prompt_env(\"RSCONNECT_SECRET\", \"Enter shinyapps.io secret: \")\n\n# Reload .env file to ensure new values are available\ndotenv::load_dot_env(\".env\")\n\n# Set account info\nrsconnect::setAccountInfo(\n  name = name,\n  token = token,\n  secret = secret\n)\n\nNow you can go to GitGuardian and mark the incident(s) as resolved. You can select the repository and click “Scan” to make sure you have removed all secrets successfully."
  },
  {
    "objectID": "blog/nmds/index.html",
    "href": "blog/nmds/index.html",
    "title": "Non-metric Multidimensional Scaling (NMDS)",
    "section": "",
    "text": "Guest lecture for Dr. Drew Gronewold’s class EAS 501.077 Multivariate Statistics for Environmental Science in 2023 Fall."
  },
  {
    "objectID": "blog/nmds/index.html#objectives",
    "href": "blog/nmds/index.html#objectives",
    "title": "Non-metric Multidimensional Scaling (NMDS)",
    "section": "Objectives",
    "text": "Objectives\n\nCompare parametric and nonparametric methods for ordination\nLearn about real-life applications of NMDS\nPractice using NMDS in R to analyze community composition\nGet familiar with a statistical test, PERMANOVA"
  },
  {
    "objectID": "blog/nmds/index.html#parametric-or-nonparametric",
    "href": "blog/nmds/index.html#parametric-or-nonparametric",
    "title": "Non-metric Multidimensional Scaling (NMDS)",
    "section": "Parametric or nonparametric?",
    "text": "Parametric or nonparametric?\nIn previous classes, we have learned about principal component analysis (PCA).\nPCA have certain assumptions, requiring the data to be * continuous, * linear, * normally distributed.\nNote that PCA involves calculating the Euclidean distances in the multidimensional space between samples.\nWhat if our data are * discrete (e.g., presence/absence, category), * nonlinear (e.g., day of year, precipitation), * not normally distributed (e.g., count of rare species)?\nDoes Euclidean distance still make sense?\n\n“Nonparametric statistics is the type of statistics that is not restricted by assumptions concerning the nature of the population from which a sample is drawn.”\n\nQuote from Wikipedia.\nPCA is a parametric method for ordination. Today, we are going to add a nonparametric method into our toolbox: non-metric multidimensional scaling (NMDS).\n\n\n\n\n\nExample NMDS plot.\n\n\n\n\nSource\nNMDS places samples that are more “similar” to each other closer in a low-dimensional space.\nAdvantages of NMDS include * accommodating multiple types of data, * being used with different measures of dissimilarity.\nHowever, be aware of the differences with PCA. Unlike principle components in PCA, the axes in NMDS do not carry a specific meaning."
  },
  {
    "objectID": "blog/nmds/index.html#applications-of-nmds",
    "href": "blog/nmds/index.html#applications-of-nmds",
    "title": "Non-metric Multidimensional Scaling (NMDS)",
    "section": "Applications of NMDS",
    "text": "Applications of NMDS\nCase 1: Phytoplankton community composition (Paper)\nTropical urban reservoirs face the problem of phytoplankton bloom, often dominated by toxic cyanobacteria. It has been hypothesized that macrophytes (water plants) can inhibit phytoplankton, presenting an opportunity for reservoir restoration.\n\n\n\n\n\nCyanobacterial blooms.\n\n\n\n\nHuisman et al. (2019)\nIn reservoirs in Singapore, we conducted a series of experiments to test macrophytes’ ability to alter phytoplankton communities and facilitate reservoir restoration.\n\n\n\n\n\nSet up of a mesocosm experiment.\n\n\n\n\nSim et al. (2021) Fig. 1\nApart from the effect of macrophytes on phytoplankton biomass, we also care about the effects on phytoplankton community composition.\n\n\n\n\n\nRelative abundance (biovolume) of phytoplankton taxa.\n\n\n\n\nSim et al. (2021) Fig. 3\nCommunity composition data is essentially multivariate. The (relative) abundance of each taxa is a variable, and there are usually many taxa. How do we analyze the response of all these variables together?\nWe used NMDS to visualize the effect of macrophyte treatment on phytoplankton community composition.\n\n\n\n\n\nNMDS of the relative biovolumes of phytoplankton genera in the control and macrophyte treatments.\n\n\n\n\nMowe et al. (2019) Fig. 6\nHere we see how the introduction of macrophyte caused a shift in phytoplankton community composition, with the shift being greater as density of macrophyte increased.\nCase 2: Grassland taxonomic, phylogenitic, and functional trait composition (Paper)\nThe trajectory of early succession communities is shaped by the plant phylogenetic and trait history. Teasing apart these two processes has important implications for restoration.\nHolding starting species richness constant, Karimi et al. planted communities of different phylogenetic diversity (PD) and functional trait diversity (FD).\n\n\n\n\n\nNMDS of taxonomic, phylogenetic and functional trait composition in two types of restoration treatments.\n\n\n\n\nKarimi et al. (2021) Fig. 4\nSimilar to case study 1, they examined how different treatments drive differences in community composition. Apart from taxonomic composition, they also analyzed responses in phylogenetic and functional trait composition.\nI highlight this study because they used a combination of data types to characterize composition.\n\n“Functional diversity was assessed using 12 continuous leaf traits, 6 categorical traits, 8 binary root traits, seed mass, a categorical habitat moisture trait and genome size.”\n\nThey also used NMDS to study changes in the three kinds of community composition.\n\n\n\n\n\nNMDS of taxonomic, phylogenetic and functional trait composition before and after restoration treatment.\n\n\n\n\nKarimi et al. (2021) Fig. 4 (Note their dissimilarity measures.)\nHere, we see that the PD treatment increased the dispersion of taxonomic and phylogenetic composition between communities (increased beta diversity) and caused directional shifts in functional trait composition (convergence)."
  },
  {
    "objectID": "blog/nmds/index.html#hands-on-community-composition-analysis",
    "href": "blog/nmds/index.html#hands-on-community-composition-analysis",
    "title": "Non-metric Multidimensional Scaling (NMDS)",
    "section": "Hands-on community composition analysis",
    "text": "Hands-on community composition analysis\nThere are really good NMDS tutorial1 tutorial 2 which I encourage you to try at home. Here I give another example with real-life data and some other visualization options.\n\nBackground: The National Ecological Observatory Network (NEON) collects ecological and biogeochemical data with standardized protocols across 81 field sites across the United States. The Woody Plant Vegetation Structure dataset (DP1.10098) describes the structure and composition of woody vegetation through the mapping, identification, and measurement of free-standing woody plants including trees, saplings, shrubs, lianas, etc.\n\nWe have downloaded NEON vegetation structure data at one site, Bartlett Experimental Forest (BART), for you to analyze the composition of woody vegetation. Please download them here.\nRead in some data frames.\n\ndat &lt;- read_rds(\"data.rds\")\n\ndf_tree &lt;- dat$vst_apparentindividual %&gt;%\n  arrange(desc(publicationDate)) %&gt;%\n  distinct(plotID, individualID, .keep_all = T) %&gt;%\n  filter(str_detect(plantStatus %&gt;% tolower(), \"live\")) %&gt;%\n  select(plotID, individualID)\n\ndf_tree_sp &lt;- dat$vst_mappingandtagging %&gt;%\n  arrange(desc(publicationDate)) %&gt;%\n  distinct(individualID, plotID, .keep_all = T) %&gt;%\n  select(individualID, plotID, taxonID, scientificName) %&gt;%\n  filter(!is.na(taxonID)) %&gt;%\n  filter(taxonID != \"2PLANT\")\n\ndf_plot &lt;- dat$vst_perplotperyear %&gt;%\n  arrange(desc(publicationDate)) %&gt;%\n  distinct(plotID, plotType, .keep_all = T) %&gt;%\n  select(plotID, plotType, nlcdClass, lon = decimalLongitude, lat = decimalLatitude)\n\nJoin the data frames. Each row is an individual tree (with unique individualID). Feel free to explore this dataset.\n\ndf &lt;- df_tree %&gt;%\n  inner_join(df_tree_sp,\n    by = c(\"individualID\", \"plotID\")\n  ) %&gt;%\n  inner_join(df_plot,\n    by = c(\"plotID\")\n  )\ndf %&gt;% head(10)\n\n     plotID            individualID taxonID                 scientificName\n1  BART_075 NEON.PLA.D01.BART.03230    FAGR        Fagus grandifolia Ehrh.\n2  BART_075 NEON.PLA.D01.BART.04011    FAGR        Fagus grandifolia Ehrh.\n3  BART_036 NEON.PLA.D01.BART.04548    FAGR        Fagus grandifolia Ehrh.\n4  BART_075 NEON.PLA.D01.BART.04702    TSCA Tsuga canadensis (L.) Carrière\n5  BART_075 NEON.PLA.D01.BART.03257    FAGR        Fagus grandifolia Ehrh.\n6  BART_075 NEON.PLA.D01.BART.03148    FAGR        Fagus grandifolia Ehrh.\n7  BART_075 NEON.PLA.D01.BART.03254    FAGR        Fagus grandifolia Ehrh.\n8  BART_075 NEON.PLA.D01.BART.04703    ACPE          Acer pensylvanicum L.\n9  BART_036 NEON.PLA.D01.BART.04554    FAGR        Fagus grandifolia Ehrh.\n10 BART_075 NEON.PLA.D01.BART.03300   BEAL2  Betula alleghaniensis Britton\n   plotType       nlcdClass       lon      lat\n1     tower     mixedForest -71.28649 44.05914\n2     tower     mixedForest -71.28649 44.05914\n3     tower deciduousForest -71.28588 44.06208\n4     tower     mixedForest -71.28649 44.05914\n5     tower     mixedForest -71.28649 44.05914\n6     tower     mixedForest -71.28649 44.05914\n7     tower     mixedForest -71.28649 44.05914\n8     tower     mixedForest -71.28649 44.05914\n9     tower deciduousForest -71.28588 44.06208\n10    tower     mixedForest -71.28649 44.05914\n\n\nProcess the joined data frame to get a community composition data frame. Each row is a community (a plot). Species names are now columns.\n\ndf_comm &lt;- df %&gt;%\n  group_by(plotID, plotType, nlcdClass, lat, lon, scientificName) %&gt;%\n  summarise(count = n()) %&gt;%\n  ungroup() %&gt;%\n  spread(key = \"scientificName\", value = \"count\", fill = 0)\ndf_comm %&gt;% head(10)\n\n# A tibble: 10 × 36\n   plotID   plotType    nlcdClass   lat   lon `Abies sp.` Acer pensylvanicum L…¹\n   &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;                  &lt;dbl&gt;\n 1 BART_001 distributed mixedFor…  44.0 -71.3           0                      2\n 2 BART_002 distributed deciduou…  44.0 -71.3           0                      6\n 3 BART_003 distributed deciduou…  44.1 -71.3           0                      0\n 4 BART_004 distributed mixedFor…  44.0 -71.3           0                      1\n 5 BART_005 distributed mixedFor…  44.1 -71.3           1                      0\n 6 BART_006 distributed deciduou…  44.1 -71.3           1                      6\n 7 BART_007 distributed mixedFor…  44.0 -71.3           0                      0\n 8 BART_010 distributed deciduou…  44.1 -71.3           0                      0\n 9 BART_011 distributed mixedFor…  44.1 -71.3           0                      0\n10 BART_012 distributed deciduou…  44.0 -71.3           0                      0\n# ℹ abbreviated name: ¹​`Acer pensylvanicum L.`\n# ℹ 29 more variables: `Acer rubrum L.` &lt;dbl&gt;, `Acer saccharinum L.` &lt;dbl&gt;,\n#   `Acer saccharum Marshall` &lt;dbl&gt;,\n#   `Acer saccharum Marshall var. saccharum` &lt;dbl&gt;, `Acer sp.` &lt;dbl&gt;,\n#   `Betula ×caerulea Blanch. var. caerulea` &lt;dbl&gt;,\n#   `Betula alleghaniensis Britton` &lt;dbl&gt;, `Betula lenta L.` &lt;dbl&gt;,\n#   `Betula papyrifera Marshall` &lt;dbl&gt;, …\n\n\nWe need to make this community composition data frame a matrix. Again, each row is a community (a plot) and each species is a column. Note that we leave out the metadata for plots.\n\nmat_comm &lt;- df_comm %&gt;%\n  select(-plotID, -plotType, -nlcdClass, -lon, -lat) %&gt;%\n  as.matrix()\nmat_comm [1:6, 1:6]\n\n     Abies sp. Acer pensylvanicum L. Acer rubrum L. Acer saccharinum L.\n[1,]         0                     2              1                   0\n[2,]         0                     6              0                   0\n[3,]         0                     0             15                   0\n[4,]         0                     1              5                   0\n[5,]         1                     0              0                   0\n[6,]         1                     6              0                   0\n     Acer saccharum Marshall Acer saccharum Marshall var. saccharum\n[1,]                       0                                      1\n[2,]                       0                                     22\n[3,]                       0                                      7\n[4,]                       0                                      0\n[5,]                       0                                      0\n[6,]                       0                                      2\n\n\nWith this community composition matrix, we can use the metaMDS function in vegan package to perform NMDS.\n\nset.seed(1)\nmds_comm &lt;- vegan::metaMDS(mat_comm, distant = \"bray\", k = 4, try = 100)\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.08885463 \nRun 1 stress 0.09016598 \nRun 2 stress 0.08885805 \n... Procrustes: rmse 0.00397431  max resid 0.01364567 \nRun 3 stress 0.08885508 \n... Procrustes: rmse 0.0001215631  max resid 0.0004105488 \n... Similar to previous best\nRun 4 stress 0.08885586 \n... Procrustes: rmse 0.0003292734  max resid 0.001099849 \n... Similar to previous best\nRun 5 stress 0.0903252 \nRun 6 stress 0.08885492 \n... Procrustes: rmse 0.0001005969  max resid 0.0003412662 \n... Similar to previous best\nRun 7 stress 0.08885847 \n... Procrustes: rmse 0.004096719  max resid 0.01408031 \nRun 8 stress 0.08885396 \n... New best solution\n... Procrustes: rmse 0.002743248  max resid 0.00947331 \n... Similar to previous best\nRun 9 stress 0.09029896 \nRun 10 stress 0.09034585 \nRun 11 stress 0.08893074 \n... Procrustes: rmse 0.009504952  max resid 0.03165667 \nRun 12 stress 0.09032054 \nRun 13 stress 0.08885345 \n... New best solution\n... Procrustes: rmse 0.002182506  max resid 0.007588686 \n... Similar to previous best\nRun 14 stress 0.08889528 \n... Procrustes: rmse 0.005338802  max resid 0.01761426 \nRun 15 stress 0.08886069 \n... Procrustes: rmse 0.003999398  max resid 0.01384676 \nRun 16 stress 0.08885448 \n... Procrustes: rmse 0.0005095852  max resid 0.001713473 \n... Similar to previous best\nRun 17 stress 0.09028926 \nRun 18 stress 0.08886793 \n... Procrustes: rmse 0.005178257  max resid 0.0181378 \nRun 19 stress 0.08885554 \n... Procrustes: rmse 0.002771664  max resid 0.009498437 \n... Similar to previous best\nRun 20 stress 0.09016291 \n*** Best solution repeated 3 times\n\nmds_comm\n\n\nCall:\nvegan::metaMDS(comm = mat_comm, k = 4, try = 100, distant = \"bray\") \nglobal Multidimensional Scaling using monoMDS\nData:     wisconsin(sqrt(mat_comm)) \nDistance: bray \nDimensions: 4 \nStress:     0.08885345 \nStress type 1, weak ties\nBest solution was repeated 3 times in 20 tries\nThe best solution was from try 13 (random start)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'wisconsin(sqrt(mat_comm))' \n\n\nWe check the stress value to find out if our NMDS has a good fit. We can also make a stressplot.\n\nmds_comm$stress\n\n[1] 0.08885345\n\nvegan::stressplot(mds_comm)\n\n\n\n\nNMDS stress plot.\n\n\n\n\nIs our stress value considered good? A rule of thumb is that stress &lt; 0.1 is good and stress &lt; 0.05 is excellent. You can increase the number of dimensions (k) to reduce stress. However, large k is usually not useful and can even be harmful.\nFor more considerations, read this chapter.\nNow we can generate a basic NMDS plot. Labels in black show communities, and labels in red show species.\n\nplot(mds_comm, type = \"t\")\n\n\n\n\nA basic NMDS plot.\n\n\n\n\nYou can see that some communities are more similar than others, and some species tend to occur together.\nWe can try to redraw the NMDS plot using ggplot. This gives you more control on the graph elements.\n\ndf_nmds_comm &lt;- mds_comm %&gt;%\n  vegan::scores(display = \"sites\") %&gt;%\n  data.frame() %&gt;%\n  bind_cols(df_comm %&gt;%\n    select(plotID, plotType, nlcdClass))\n\ndf_nmds_sp &lt;- mds_comm %&gt;%\n  vegan::scores(display = \"species\") %&gt;%\n  data.frame() %&gt;%\n  rownames_to_column(var = \"species\")\n\nggplot(df_nmds_comm, aes(x = NMDS1, y = NMDS2)) +\n  geom_point() +\n  ggrepel::geom_text_repel(data = df_nmds_sp, aes(NMDS1, NMDS2, label = species), color = \"dark grey\") +\n  ggthemes::theme_few()\n\n\n\n\nA NMDS plot drawn using ggplot.\n\n\n\n\nWe ca draw ellipses based on existing grouping of these communities. In experiment, we can draw ellipses for control and treatment. In observations, we can draw ellipses for different time points. Here, I draw ellipses for communities from different land cover types.\n\nggplot(df_nmds_comm, aes(x = NMDS1, y = NMDS2, col = nlcdClass)) +\n  geom_point() +\n  stat_ellipse() +\n  ggrepel::geom_text_repel(data = df_nmds_sp, aes(NMDS1, NMDS2, label = species), color = \"dark grey\") +\n  ggthemes::theme_few() +\n  coord_equal()\n\n\n\n\nNMDS plot showing composition of communities from different land cover types.\n\n\n\n\nYou can see some differences in the composition of communities from different land cover types. Note that we used the first two axes of NMDS. What if we use another two axes?\n\nggplot(df_nmds_comm, aes(x = NMDS3, y = NMDS4, col = nlcdClass)) +\n  geom_point() +\n  stat_ellipse() +\n  ggrepel::geom_text_repel(data = df_nmds_sp, aes(NMDS1, NMDS2, label = species), color = \"dark grey\") +\n  ggthemes::theme_few() +\n  coord_equal()\n\n\n\n\nNMDS plot showing composition of communities from different land cover types, using NMDS3 and NMDS4.\n\n\n\n\nWe still see some differences using NMDS3 and NMDS4, but perhaps less distinct compared to when we used NMDS1 and NMDS2.\nNEON has two types of plots, distributed and tower. Their sampling methods differ. Let’s see their difference.\n\nggplot(df_nmds_comm, aes(x = NMDS1, y = NMDS2, col = plotType)) +\n  geom_point() +\n  stat_ellipse() +\n  ggrepel::geom_text_repel(data = df_nmds_sp, aes(NMDS1, NMDS2, label = species), color = \"dark grey\") +\n  ggthemes::theme_few() +\n  coord_equal()\n\n\n\n\nNMDS plot showing composition of communities from different plot types.\n\n\n\n\nCommunities from distributed plots seem to be more dispersed? The reason might be tower plots have a more constrained sampling area."
  },
  {
    "objectID": "blog/nmds/index.html#permanova",
    "href": "blog/nmds/index.html#permanova",
    "title": "Non-metric Multidimensional Scaling (NMDS)",
    "section": "PERMANOVA",
    "text": "PERMANOVA\nWe have done some visualization that hopefully help us intuitively see the similarity and differences between groups of samples. What if we are asked to statistically quantify the differences between these groups? How can we get a p value?\nPermutational multivariate analysis of variance (PERMANOVA) is a nonparametric multivariate statistical permutation test. * A significant p value indicates that the two groups are different in the their centroids OR dispersion in the multidimensional space. * It is similar to ANOVA, but it does not have many assumptions except exchangeability (usually satisfied).\n(The two case studies we introduced both used PERMANOVA.)\nIn practice, we can easily use the adonis2 function from the vegan package.\n\nset.seed(1)\nres_permanova &lt;- vegan::adonis2(mat_comm ~ nlcdClass, data = df_comm, permutations = 9999)\nres_permanova\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 9999\nvegan::adonis2(formula = mat_comm ~ nlcdClass, data = df_comm, permutations = 9999)\n         Df SumOfSqs      R2      F Pr(&gt;F)    \nModel     2   2.1141 0.31126 8.3605  1e-04 ***\nResidual 37   4.6781 0.68874                  \nTotal    39   6.7922 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nset.seed(1)\nres_permanova &lt;- vegan::adonis2(mat_comm ~ plotType, data = df_comm, permutations = 9999)\nres_permanova\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 9999\nvegan::adonis2(formula = mat_comm ~ plotType, data = df_comm, permutations = 9999)\n         Df SumOfSqs      R2      F Pr(&gt;F)    \nModel     1   1.0483 0.15434 6.9352  6e-04 ***\nResidual 38   5.7439 0.84566                  \nTotal    39   6.7922 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nset.seed(1)\nres_permanova &lt;- vegan::adonis2(mat_comm ~ nlcdClass * plotType, data = df_comm, permutations = 9999)\nres_permanova\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 9999\nvegan::adonis2(formula = mat_comm ~ nlcdClass * plotType, data = df_comm, permutations = 9999)\n         Df SumOfSqs      R2      F Pr(&gt;F)    \nModel     4   3.1207 0.45946 7.4374  1e-04 ***\nResidual 35   3.6715 0.54054                  \nTotal    39   6.7922 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCan you try to interpret the results of these three PERMANOVA test that correspond to the three NMDS plots above? Are the differences that we observed from NMDS plots statistically significant?"
  },
  {
    "objectID": "blog/github/index.html",
    "href": "blog/github/index.html",
    "title": "GitHub",
    "section": "",
    "text": "This is a practical introduction to using Git and GitHub.\nGuest lectures in UCSC ENVS 280 Data Science for the Environment and UofM IGCB R Session."
  },
  {
    "objectID": "blog/github/index.html#objectives",
    "href": "blog/github/index.html#objectives",
    "title": "GitHub",
    "section": "Objectives",
    "text": "Objectives\n\nKnow the importance and rationale of version control and Git\nLearn some basic commands in Git\nSet up a GitHub account and your project\nPlay with branch and fork"
  },
  {
    "objectID": "blog/github/index.html#version-control",
    "href": "blog/github/index.html#version-control",
    "title": "GitHub",
    "section": "Version control",
    "text": "Version control\n\nWhat is “version control,” and why should you care? Version control is a system that records changes to a file or set of files over time so that you can recall specific versions later.\n\nFrom Pro Git book\nFor example, you can * Revert selected files back to a previous state * Revert the entire project back to a previous state * Compare changes over time * See who last modified something that might be causing a problem and when\nWe now commonly use Distributed Version Control Systems (DVCSs), such as Git. In DVCSs, each client server has a full a full backup of the repository, including the history.\n\n\n\nDistributed version control diagram. Source: Pro Git Book.\n\n\n\nGit is a distributed version-control system for tracking changes in source code. It was designed for coordinating work among programmers, but it can be used to track changes in any set of files.\n\nSource\nGitHub and Bitbucket are two of the largest web-based hosting services, but we will focus on GitHub here."
  },
  {
    "objectID": "blog/github/index.html#git-workflow",
    "href": "blog/github/index.html#git-workflow",
    "title": "GitHub",
    "section": "Git workflow",
    "text": "Git workflow\n\n\n\nGit workflow.\n\n\nSource\nLocations: work space, staging area, local repository, remote repository\nActions: add, commit, push, pull\nA typical workflow * Pull changes in remote repo to work space * Make changes in work space * Add changes to the staging area * Commit changes to local repo * Push changes to remote repo\nExample commands look like this. Refer to Git Basic or GPT for more commands.\n\ngit pull\ngit add -A\ngit commit -m \"[commit message]\"\ngit push\n\nNote: Always try to pull before you make changes locally to avoid conflicts. There would be ways to resolve conflicts if they do occur though."
  },
  {
    "objectID": "blog/github/index.html#set-up-your-git-repo",
    "href": "blog/github/index.html#set-up-your-git-repo",
    "title": "GitHub",
    "section": "Set up your Git repo",
    "text": "Set up your Git repo\nPrerequisite 1: Register for an account on GitHub here.\nPrerequisite 2: Generate a token at Settings &gt; Developer Settings &gt; Personal access tokens &gt; Tokens (classic) &gt; Generate new token. Enable the access you need but for testing today enable “repo” at least. Save the generated token somewhere you know and safe. You will not be able to see this token from GitHub again. Read more instructions about token here. Note: GitHub will soon require 2FA, so you can set that up too if you have time.\nPrerequisite 3: Install Git if your computer does not have it already. School HPC servers usually have it installed. To check if you have Git installed, run in your terminal\n\ngit --version\n\nInstallation differs based on your system. See instructions here.\nGo to GitHub and create new repository. I usually make it Private first, add a README file, add .gitignore(using R template if you are an R user), and choose a license (e.g., MIT).\nClick on green “&lt;&gt; Code” button and find the web URL of your repo.\nConfigure git by running the following commands in the terminal.\n\ngit config --global user.name [Your name]\ngit config --global user.email [Your email]\ngit config --global credential.helper 'store --file ~/.git-credentials'\n# Git will save your credentials in ~/.git-credentials when you next enter them\n# Change path of credential file as you wish\n\nClone the repo from remote to work space.\n\ncd [directory where you want to set up your work space]\ngit clone https://&lt;username&gt;:&lt;token&gt;@github.com/&lt;username or organization name&gt;/&lt;repo name&gt;.git\n# This is adapted from the web URL of the repo, with credentials added\n# If you already have ~/.git-credentials set up, you can just use\ngit clone https://github.com/&lt;username or organization name&gt;/&lt;repo name&gt;.git\n\nMake it an R project. In Rstudio, create a new project from your work space. After you are in the project, you should see a tab called “Git” on the top right.\nMake some changes in your work space, such as copying your R files into your work space.\nNow we run some git commands in the terminal. Note: Most of these actions can be done with the RStudio GUI in the Git tab, you can play with them, but here I introduce the commands for better control and reproducibility.\nMake sure your directory is at your work space.\n\ncd [directory of your work space]\n\nCheck the status.\n\ngit status\n\nStage all changes.\n\ngit add -A\n\nCommit changes.\n\ngit commit -m \"init commit\"\n\nPush changes.\n\ngit push\n\nFor help.\n\ngit --help\n\nLook at the changes in your remote repo on GitHub.\n“git clone” applies to the first time you set up the work space on this computer. Next time, you can directly pull from remote repo.\n\ngit pull\n\nIf you have multiple contributors to this repo, or if you use multiple computers, you might not pull changes in time and run into conflicts. A common scenario is that you push but get a fatal error. You need to pull at this point. If there is no conflict, you can write a commit message for merging the remote and local repo, and push again. Sometimes there are conflicts. The files with conflicts will be changed, with some lines looking like this.\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nold code\n=======\nnew code\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; [branch name]\n\nYou have to manually edit the files, add, commit, and push again."
  },
  {
    "objectID": "blog/github/index.html#branch-and-fork",
    "href": "blog/github/index.html#branch-and-fork",
    "title": "GitHub",
    "section": "Branch and fork",
    "text": "Branch and fork\nIn order not to manage a repo with multiple collaborators more easily, you can use branch or forks.\nIn the GitHub page of your repo, you can create a new branch. After you pull your repo, you can switch between projects using\n\ngit checkout [another branch]\n\nYou can make changes, add, commit, and push as usual.\nWhen your branch is working well, you can merge your branch into another branch, perhaps the main.\n\ngit checkout [main branch]\ngit pull\ngit merge [your branch]\n\nYou might have to resolve conflicts at this point. After this, add, commit, and push.\nIf you do not work very closely as a team or would like a bit more independence, you can also fork your own copy of a repo. You can work with the fork in the same way as an independent repo, but you can “Sync” with the parent repo or “Contribute” to the parent repo."
  },
  {
    "objectID": "blog/github/index.html#others",
    "href": "blog/github/index.html#others",
    "title": "GitHub",
    "section": "Others",
    "text": "Others\nGitHub have other interesting features such as “Issues,”“GitHub Actions,” and “GitHub pages.” Please enjoy exploring them."
  }
]